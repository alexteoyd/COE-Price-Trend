# ğŸš— Singapore COE Data Engineering Pipeline

## ğŸ§  Overview

This project demonstrates a complete data engineering pipeline that extracts, processes, and stores data related to Singaporeâ€™s **Certificate of Entitlement (COE)** system. The pipeline integrates data from a public API and a PDF source to build a **dimensional model** in a **PostgreSQL** database using a **star schema**.

It showcases skills in:

- Data ingestion from multiple sources (API + PDF)
- Data cleaning and transformation
- Dimensional modeling and relational schema design
- Automated ETL using Python
- Reproducibility and environment setup

---

## ğŸ”‘ Key Features

- **ğŸ“¥ Multi-Source Ingestion**  
  Extracts COE bidding results from [Data.gov.sg API](https://data.gov.sg) and parses an unstructured PDF (e.g., from NCCS) for vehicle category reference data.

- **ğŸ§¹ Data Transformation & Modeling**  
  Raw data is reshaped into a **star schema** with:
  - `fact_coe_monthly`
  - `date_dim`
  - `category_dim`
  - `car_reference`

- **ğŸ’¾ Database Loading**  
  Loads structured data into a **PostgreSQL** database using SQLAlchemy and pandas.

- **ğŸ” Reproducible Environment**  
  Uses `.env` for secret management and `requirements.txt` for environment setup.

---

## ğŸ—‚ï¸ Project Structure

coe-analysis/
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ coe_eda.ipynb # Exploratory Data Analysis
â”œâ”€â”€ sql/
â”‚ â””â”€â”€ schema.sql # Database schema (DDL)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ etl_pipeline.py # ETL logic
â”‚ â””â”€â”€ main.py # Main entry script
â”œâ”€â”€ .env.example # Environment variable template
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation

---

## ğŸ–¥ï¸ How to Run the Project

### âœ… Prerequisites

- Python 3.10+
- PostgreSQL installed (locally or via Docker)

### ğŸ“¦ Setup Instructions

1. **Clone the repo**  
   ```bash
   git clone https://github.com/your-username/coe-analysis.git
   cd coe-analysis

2. **Create a virtual environment and install dependencies**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    pip install -r requirements.txt

3. **Set up your environment variables**
Create a .env file in the root directory based on .env.example:
   ```env
   API_KEY=your_data_gov_sg_api_key
   PG_PASSWORD=your_postgresql_password

4. **Run the ETL pipeline**
   ```bash
   python src/main.py


This script will:
- Create the PostgreSQL database and tables
- Ingest data from the API and PDF
- Transform and load data into the star schema

## ğŸ“Š Example Insights
- ğŸ“‰ COE prices trend higher in Q2 and Q3, suggesting seasonal spikes
- ğŸš˜ Category B (larger vehicles) consistently has the highest average COE
- ğŸ“… Most affordable periods for COE bidding are typically in early Q1
- ğŸ’¡ Data supports smarter decision-making for vehicle purchases and forecasting

â¡ï¸ See notebooks/coe_eda.ipynb
 for data exploration and visualization.

## ğŸ“š What I Learned
- How to build a modular, production-ready ETL pipeline in Python
- Parsing unstructured PDF documents into structured data
- Creating star schemas and dimensional models for analytics
- Managing secure credentials and environment reproducibility
- Writing clean, version-controlled code in a professional repo structure


